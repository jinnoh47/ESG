{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6763e72",
   "metadata": {},
   "source": [
    "# ESGReveal:An LLM-based approach for extracting structured data from ESG reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a95c86-2c29-4ffc-8d18-86c2ca1796f1",
   "metadata": {},
   "source": [
    "## 1. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbef8e",
   "metadata": {},
   "source": [
    "#### I used **LangChain** as the main framework to implement the functionalities described in the paper. LangChain provides a modular approach to working with Large Language Models (LLMs), making it highly adaptable for tasks such as document preprocessing, metadata tagging, and retrieval-augmented generation (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903d74e",
   "metadata": {},
   "source": [
    "\n",
    "#### Why LangChain?\n",
    "\n",
    "1. **Ease of Integration with LLMs**:\n",
    "   - The paper relies heavily on LLMs for extracting and structuring ESG data. LangChain simplifies the integration process with APIs like OpenAI or other LLM providers.\n",
    "\n",
    "2. **RAG Framework**:\n",
    "   - LangChain provides native support for **Retrieval-Augmented Generation (RAG)** workflows, which align closely with the paper's emphasis on metadata tagging, data retrieval, and structured response generation.\n",
    "\n",
    "3. **Document Preprocessing**:\n",
    "   - The framework supports easy preprocessing of unstructured documents, like PDF files, using loaders such as `PyPDFLoader`.\n",
    "\n",
    "4. **Flexibility and Scalability**:\n",
    "   - LangChain is modular, allowing me to add custom logic for metadata tagging, question answering, and document segmentation as per the paper's requirements.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e462870",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:49.173383Z",
     "iopub.status.busy": "2024-12-06T01:14:49.173045Z",
     "iopub.status.idle": "2024-12-06T01:14:49.485969Z",
     "shell.execute_reply": "2024-12-06T01:14:49.485120Z",
     "shell.execute_reply.started": "2024-12-06T01:14:49.173364Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "# add parent path as sys path (to import python files)\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bedf67ab-a514-4aa4-b47b-d566993043bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:50.306839Z",
     "iopub.status.busy": "2024-12-06T01:14:50.306402Z",
     "iopub.status.idle": "2024-12-06T01:14:51.143654Z",
     "shell.execute_reply": "2024-12-06T01:14:51.142906Z",
     "shell.execute_reply.started": "2024-12-06T01:14:50.306818Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Open_ai_apikey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755724d8-2556-428c-a8b9-fa7634edb0cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:51.144945Z",
     "iopub.status.busy": "2024-12-06T01:14:51.144768Z",
     "iopub.status.idle": "2024-12-06T01:14:51.149213Z",
     "shell.execute_reply": "2024-12-06T01:14:51.148583Z",
     "shell.execute_reply.started": "2024-12-06T01:14:51.144928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 디렉토리: /home/jovyan/samchully/jhn\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# checking directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"현재 디렉토리: {current_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b59e339-b129-4e27-b280-1943819ab354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:52.238054Z",
     "iopub.status.busy": "2024-12-06T01:14:52.236662Z",
     "iopub.status.idle": "2024-12-06T01:14:52.243563Z",
     "shell.execute_reply": "2024-12-06T01:14:52.242751Z",
     "shell.execute_reply.started": "2024-12-06T01:14:52.238008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경된 디렉토리: /home/jovyan/samchully/jhn/esg_inc\n"
     ]
    }
   ],
   "source": [
    "new_directory = \"/home/jovyan/samchully/jhn/esg_inc\"\n",
    "os.chdir(new_directory)\n",
    "\n",
    "# checking altered directory\n",
    "print(f\"변경된 디렉토리: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb51459e-ba09-460c-a118-2ee91a83d5a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:53.649939Z",
     "iopub.status.busy": "2024-12-06T01:14:53.649615Z",
     "iopub.status.idle": "2024-12-06T01:14:53.910915Z",
     "shell.execute_reply": "2024-12-06T01:14:53.909996Z",
     "shell.execute_reply.started": "2024-12-06T01:14:53.649912Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "directory_path = new_directory\n",
    "\n",
    "pdf_files = [\n",
    "    os.path.join(directory_path, file) \n",
    "    for file in os.listdir(directory_path) \n",
    "    if file.endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "def clean_invalid_unicode(text):\n",
    "    \"\"\"\n",
    "    문자열에서 유효하지 않은 Unicode 대리 페어를 제거합니다.\n",
    "    \"\"\"\n",
    "    return text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b3c072-c9cf-470b-8cfd-925aa04350a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:54.441958Z",
     "iopub.status.busy": "2024-12-06T01:14:54.441576Z",
     "iopub.status.idle": "2024-12-06T01:14:54.556883Z",
     "shell.execute_reply": "2024-12-06T01:14:54.556056Z",
     "shell.execute_reply.started": "2024-12-06T01:14:54.441923Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "# Define default values\n",
    "DEFAULT_CHUNK_SIZE = 1000\n",
    "DEFAULT_CHUNK_OVERLAP = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb96bad",
   "metadata": {},
   "source": [
    "## 2. Parse Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a46d63",
   "metadata": {},
   "source": [
    "#### LangChain’s PyPDFLoader is used to extract text efficiently from PDFs with standard text encoding, preserving metadata such as page numbers and sources. For scanned or complex PDFs where text extraction fails, OCR serves as a fallback to ensure complete data retrieval. This hybrid approach validates content using custom functions to clean irrelevant symbols and assess quality, determining whether to use direct extraction or OCR. Text is processed page by page, cleaned, and combined with metadata for usability. Future improvements include optimizing OCR for multilingual text, enhancing scalability for large datasets, and enriching metadata with advanced features like table detection. This ensures reliable text extraction across diverse document types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a36dd16-7d0f-4931-9f4b-e3d80224b4f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:55.827522Z",
     "iopub.status.busy": "2024-12-06T01:14:55.826656Z",
     "iopub.status.idle": "2024-12-06T01:14:55.844314Z",
     "shell.execute_reply": "2024-12-06T01:14:55.843317Z",
     "shell.execute_reply.started": "2024-12-06T01:14:55.827458Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_non_korean_english_symbols(text):\n",
    "    return re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "def cid_ratio(text):\n",
    "    if len(text) <= 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return len(re.findall(r\"\\(cid:\\d+\\)\", text)) / len(text)\n",
    "\n",
    "def extract_partial_text(pdf_path, max_pages=None, max_chars=None):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = len(doc)\n",
    "    max_pages = min(max_pages or total_pages, total_pages)\n",
    "    extracted_text = \"\"\n",
    "    for page_num in range(max_pages):\n",
    "        page = doc[page_num]\n",
    "        extracted_text += page.get_text()\n",
    "        if max_chars and len(extracted_text) >= max_chars:\n",
    "            return extracted_text[:max_chars].strip()\n",
    "    return extracted_text.strip()\n",
    "\n",
    "def pdf_cid_ratio(file_path):\n",
    "    try:\n",
    "        text = extract_partial_text(file_path, max_chars=10240)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {e}\")\n",
    "        return None\n",
    "    return cid_ratio(text) if text.strip() else -1\n",
    "\n",
    "def pdf_clean_text_ratio(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    orig_text = \"\"\n",
    "    clean_text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()\n",
    "        orig_text += text\n",
    "        clean_text += remove_non_korean_english_symbols(text)\n",
    "        if len(orig_text) > 10240:\n",
    "            break\n",
    "    return len(clean_text) / len(orig_text) if len(orig_text) > 0 else -1\n",
    "\n",
    "def pdf_is_valid(filename):\n",
    "    retval1 = pdf_cid_ratio(filename)\n",
    "    if retval1 is None:\n",
    "        return False\n",
    "    retval2 = pdf_clean_text_ratio(filename)\n",
    "    return retval1 < 0.1 and retval2 > 0.5\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text_with_pages = []\n",
    "    if pdf_is_valid(pdf_path):\n",
    "        doc = fitz.open(pdf_path)\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            text = remove_non_korean_english_symbols(page.get_text())\n",
    "            text_with_pages.append((page_num + 1, text))\n",
    "    else:\n",
    "        pages = convert_from_path(pdf_path)\n",
    "        for page_num, page in enumerate(pages):\n",
    "            text = pytesseract.image_to_string(page, lang=\"eng+kor\")\n",
    "            text_with_pages.append((page_num + 1, text))\n",
    "    return text_with_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae976935-ea19-4f88-bae6-bee523d4b197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:56.369353Z",
     "iopub.status.busy": "2024-12-06T01:14:56.368933Z",
     "iopub.status.idle": "2024-12-06T01:14:56.570914Z",
     "shell.execute_reply": "2024-12-06T01:14:56.570086Z",
     "shell.execute_reply.started": "2024-12-06T01:14:56.369316Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca1681c-e390-4ce5-84ed-e738f2ca328c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:56.803794Z",
     "iopub.status.busy": "2024-12-06T01:14:56.802893Z",
     "iopub.status.idle": "2024-12-06T01:14:56.818543Z",
     "shell.execute_reply": "2024-12-06T01:14:56.817752Z",
     "shell.execute_reply.started": "2024-12-06T01:14:56.803756Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal, List, Optional\n",
    "\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "class ESGProperties(BaseModel):\n",
    "    # category\n",
    "    aspect: Literal[\"Environmental\", \"Social\", \"Governance\"]\n",
    "    # performance indicator\n",
    "    kpi: str = Field(description=\"The key performance indicator\")\n",
    "    # topic\n",
    "    topic: Optional[str] = Field(description=\"The topic under the KPI\")\n",
    "    # metic quantity\n",
    "    quantity: Optional[dict] = Field(\n",
    "        description=(\n",
    "            \"A dictionary containing a numeric value and its corresponding unit. \"\n",
    "            \"Common metrics include environmental,social,financial, and operational data. \"\n",
    "            \"Values should be numeric, and units should reflect the measurement context \"\n",
    "            \"such as tons, percentages, monetary units, or hours.\"\n",
    "        ),\n",
    "        example=[\n",
    "            {\"value\": 1500, \"unit\": \"metric tons\"},  # \n",
    "            {\"value\": 85, \"unit\": \"percent\"},  # %\n",
    "            {\"value\": 200000, \"unit\": \"USD\"},  # currency\n",
    "            {\"value\": 1000, \"unit\": \"m³\"},  # \n",
    "            {\"value\": 40, \"unit\": \"hours\"},  # hours\n",
    "            {\"value\": 0, \"unit\": \"cases\"},  # cases\n",
    "        ]\n",
    "    )\n",
    "    # addtional information\n",
    "    search_terms: Optional[List[str]] = Field(\n",
    "        description=\"Search terms related to the document\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3b495a-e0b1-445a-901c-c15a1360db57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:57.646481Z",
     "iopub.status.busy": "2024-12-06T01:14:57.645998Z",
     "iopub.status.idle": "2024-12-06T01:14:57.820438Z",
     "shell.execute_reply": "2024-12-06T01:14:57.819588Z",
     "shell.execute_reply.started": "2024-12-06T01:14:57.646440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1049203/3384632203.py:7: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_transformers.openai_functions import (\n",
    "    create_metadata_tagger,\n",
    ")\n",
    "# OpenAI 모델 설정\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "# 메타데이터 태거 생성\n",
    "esg_tagger = create_metadata_tagger(ESGProperties, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb349b9-1f0f-490c-84a2-44707fe6d84d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:14:58.778005Z",
     "iopub.status.busy": "2024-12-06T01:14:58.777621Z",
     "iopub.status.idle": "2024-12-06T01:15:14.604247Z",
     "shell.execute_reply": "2024-12-06T01:15:14.603348Z",
     "shell.execute_reply.started": "2024-12-06T01:14:58.777971Z"
    }
   },
   "outputs": [],
   "source": [
    "# pdf file lists \n",
    "all_documents = []\n",
    "\n",
    "for pdf_file in pdf_files:  \n",
    "    try:\n",
    "        loader = PyPDFLoader(file_path=pdf_file)\n",
    "        documents = loader.load()\n",
    "\n",
    "        for doc in documents:\n",
    "            # if there is no page content or empty page content try to extract text using OCR\n",
    "            if not doc.page_content or not doc.page_content.strip():\n",
    "                # text extraction using OCR\n",
    "                text_with_pages = extract_text_from_pdf(pdf_file)\n",
    "\n",
    "                # OCR mapping to the document\n",
    "                for page_num, text in text_with_pages:\n",
    "                    if page_num == doc.metadata.get(\"page\"):\n",
    "                        ocr_doc = Document(\n",
    "                            page_content=text,\n",
    "                            metadata={\"page\": page_num, \"source\": pdf_file}\n",
    "                        )\n",
    "                        # append to all documents\n",
    "                        all_documents.append(ocr_doc)\n",
    "                        break  # complete mapping on the page\n",
    "            else:\n",
    "                # no need to extract text using OCR\n",
    "                all_documents.append(doc)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994520fe",
   "metadata": {},
   "source": [
    "## 3. Meta Data tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3dafc6",
   "metadata": {},
   "source": [
    "#### Configure ESG metrics and incorporate metadata from each metric into code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63c4e39e-d8e6-4e87-83e8-86c1039421a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:16:29.648107Z",
     "iopub.status.busy": "2024-12-06T01:16:29.647708Z",
     "iopub.status.idle": "2024-12-06T01:27:25.795158Z",
     "shell.execute_reply": "2024-12-06T01:27:25.793999Z",
     "shell.execute_reply.started": "2024-12-06T01:16:29.648074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 106 documents successfully.\n"
     ]
    }
   ],
   "source": [
    "tagged_documents = []\n",
    "for doc in all_documents:\n",
    "    try:\n",
    "        result = esg_tagger.transform_documents([doc])\n",
    "        tagged_documents.extend(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tagging document: {doc.metadata}\")\n",
    "        print(e)\n",
    "\n",
    "# 최종 결과 확인\n",
    "print(f\"Tagged {len(tagged_documents)} documents successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a23dd2-d1e1-4b81-8d58-49058162889e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:25.045227Z",
     "iopub.status.busy": "2024-12-06T01:32:25.044359Z",
     "iopub.status.idle": "2024-12-06T01:32:25.049801Z",
     "shell.execute_reply": "2024-12-06T01:32:25.048877Z",
     "shell.execute_reply.started": "2024-12-06T01:32:25.045171Z"
    }
   },
   "outputs": [],
   "source": [
    "#tagged_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "929a60e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:25.567066Z",
     "iopub.status.busy": "2024-12-06T01:32:25.566602Z",
     "iopub.status.idle": "2024-12-06T01:32:25.577487Z",
     "shell.execute_reply": "2024-12-06T01:32:25.576971Z",
     "shell.execute_reply.started": "2024-12-06T01:32:25.567046Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 토큰 최적화 하면서, 텍스트 구조를 최대한 보존하면서 분할하기 때문\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10f509e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:26.245826Z",
     "iopub.status.busy": "2024-12-06T01:32:26.245411Z",
     "iopub.status.idle": "2024-12-06T01:32:26.279229Z",
     "shell.execute_reply": "2024-12-06T01:32:26.278698Z",
     "shell.execute_reply.started": "2024-12-06T01:32:26.245794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "split_documents = []\n",
    "\n",
    "for document in tagged_documents:\n",
    "    # tuple에서 'content'와 'metadata'를 가져와 Document 객체로 변환\n",
    "    if isinstance(document, dict):  # OCR 처리 결과일 경우\n",
    "        doc = Document(page_content=document[\"content\"], metadata={\"page\": document[\"page\"]})\n",
    "    elif isinstance(document, Document):  # 이미 Document 객체일 경우 그대로 사용\n",
    "        doc = document\n",
    "        doc.page_content = clean_invalid_unicode(doc.page_content)\n",
    "    else:  # document가 문자열일 경우\n",
    "        doc = Document(page_content=clean_invalid_unicode(document), metadata={})\n",
    "    split_docs = text_splitter.split_documents([doc]) #list 형태로 해야함 \n",
    "    #print(split_docs)\n",
    "    for split_docs in split_docs:\n",
    "        split_documents.append(split_docs)\n",
    "    \n",
    "print(len(split_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "497c22c3-1c27-43a8-9860-3a80cc1bb499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:33.295397Z",
     "iopub.status.busy": "2024-12-06T01:32:33.294955Z",
     "iopub.status.idle": "2024-12-06T01:32:33.304676Z",
     "shell.execute_reply": "2024-12-06T01:32:33.304278Z",
     "shell.execute_reply.started": "2024-12-06T01:32:33.295345Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 메타데이터 저장\n",
    "with open(\"esg_metadata.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump([doc.metadata for doc in tagged_documents], file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ecb651",
   "metadata": {},
   "source": [
    "## 4. Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ffaeb4-9f75-44db-a576-520aec0e9ccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:33.936647Z",
     "iopub.status.busy": "2024-12-06T01:32:33.936294Z",
     "iopub.status.idle": "2024-12-06T01:32:33.966875Z",
     "shell.execute_reply": "2024-12-06T01:32:33.966391Z",
     "shell.execute_reply.started": "2024-12-06T01:32:33.936616Z"
    }
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6b0f8bb-d6ce-4cd8-8c71-66805b1b9c9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:34.400987Z",
     "iopub.status.busy": "2024-12-06T01:32:34.400588Z",
     "iopub.status.idle": "2024-12-06T01:32:34.432907Z",
     "shell.execute_reply": "2024-12-06T01:32:34.432481Z",
     "shell.execute_reply.started": "2024-12-06T01:32:34.400954Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5d0874a-c086-4516-ba96-ce5b41b47139",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:34.763207Z",
     "iopub.status.busy": "2024-12-06T01:32:34.762849Z",
     "iopub.status.idle": "2024-12-06T01:32:39.624642Z",
     "shell.execute_reply": "2024-12-06T01:32:39.623715Z",
     "shell.execute_reply.started": "2024-12-06T01:32:34.763167Z"
    }
   },
   "outputs": [],
   "source": [
    "# DB 생성\n",
    "db = FAISS.from_documents(documents=split_documents, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36c905cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:41.345835Z",
     "iopub.status.busy": "2024-12-06T01:32:41.345517Z",
     "iopub.status.idle": "2024-12-06T01:32:41.350365Z",
     "shell.execute_reply": "2024-12-06T01:32:41.349644Z",
     "shell.execute_reply.started": "2024-12-06T01:32:41.345810Z"
    }
   },
   "outputs": [],
   "source": [
    "# FAISS 인덱스를 파일에 저장\n",
    "faiss.write_index(db.index, \"faiss_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ed84c5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:42.622598Z",
     "iopub.status.busy": "2024-12-06T01:32:42.621949Z",
     "iopub.status.idle": "2024-12-06T01:32:42.627314Z",
     "shell.execute_reply": "2024-12-06T01:32:42.626444Z",
     "shell.execute_reply.started": "2024-12-06T01:32:42.622563Z"
    }
   },
   "outputs": [],
   "source": [
    "# 파일에서 FAISS 인덱스 로드\n",
    "index = faiss.read_index(\"faiss_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dc9e7bf-6e12-48ac-a564-b514f56cc9f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T01:32:45.597885Z",
     "iopub.status.busy": "2024-12-06T01:32:45.597478Z",
     "iopub.status.idle": "2024-12-06T01:32:45.602856Z",
     "shell.execute_reply": "2024-12-06T01:32:45.601631Z",
     "shell.execute_reply.started": "2024-12-06T01:32:45.597854Z"
    }
   },
   "outputs": [],
   "source": [
    "# 문서 저장소 ID 확인\n",
    "#db.index_to_docstore_id\n",
    "\n",
    "# 문서 저장내용\n",
    "#db.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1c6d1c8-0037-4a5d-a5a6-5c990c03bfd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:00:08.756227Z",
     "iopub.status.busy": "2024-12-06T02:00:08.755953Z",
     "iopub.status.idle": "2024-12-06T02:00:08.781230Z",
     "shell.execute_reply": "2024-12-06T02:00:08.780783Z",
     "shell.execute_reply.started": "2024-12-06T02:00:08.756208Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "directory_path = \"/home/jovyan/samchully/jhn/output\"\n",
    "\n",
    "# 디렉터리 내의 모든 CSV 파일 필터링\n",
    "csv_files = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.endswith('.csv')]\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    loader = CSVLoader(file_path = csv_file)\n",
    "    data = loader.load()\n",
    "    all_data.append(data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef9c98e7-b5a0-46c8-9a88-1646018f6089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:01:08.736402Z",
     "iopub.status.busy": "2024-12-06T02:01:08.735961Z",
     "iopub.status.idle": "2024-12-06T02:01:08.742321Z",
     "shell.execute_reply": "2024-12-06T02:01:08.741263Z",
     "shell.execute_reply.started": "2024-12-06T02:01:08.736355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8207896-05ab-4966-8677-1ccaeb72de51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:06:05.516705Z",
     "iopub.status.busy": "2024-12-06T02:06:05.516290Z",
     "iopub.status.idle": "2024-12-06T02:41:04.736013Z",
     "shell.execute_reply": "2024-12-06T02:41:04.734763Z",
     "shell.execute_reply.started": "2024-12-06T02:06:05.516672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged 0 documents successfully.\n"
     ]
    }
   ],
   "source": [
    "tagged_tables = []\n",
    "for doc in all_data:\n",
    "    try:\n",
    "        result = esg_tagger.transform_documents(doc)\n",
    "        tagged_documents.extend(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error tagging document: {doc.metadata}\")\n",
    "        print(e)\n",
    "\n",
    "# 최종 결과 확인\n",
    "print(f\"Tagged {len(tagged_tables)} documents successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6210e1b1-bcf1-4232-b2c5-c1bf9515e172",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:45:46.791414Z",
     "iopub.status.busy": "2024-12-06T02:45:46.790984Z",
     "iopub.status.idle": "2024-12-06T02:45:46.822254Z",
     "shell.execute_reply": "2024-12-06T02:45:46.821412Z",
     "shell.execute_reply.started": "2024-12-06T02:45:46.791381Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# all_data는 리스트 형태로 여러 파일의 데이터를 포함하므로 이를 문자열로 변환\n",
    "all_text = \"\\n\".join(str(data) for data in all_data)  # 데이터를 문자열로 변환 및 병합\n",
    "\n",
    "# 텍스트 분할기 생성\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# 텍스트 분할\n",
    "texts = text_splitter.split_text(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a364ab5-11fc-4227-a8e1-d25ab03ae0f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:48:29.449104Z",
     "iopub.status.busy": "2024-12-06T02:48:29.448703Z",
     "iopub.status.idle": "2024-12-06T02:48:32.696449Z",
     "shell.execute_reply": "2024-12-06T02:48:32.695535Z",
     "shell.execute_reply.started": "2024-12-06T02:48:29.449072Z"
    }
   },
   "outputs": [],
   "source": [
    "db.add_texts(texts)\n",
    "# FAISS 인덱스를 파일에 저장\n",
    "faiss.write_index(db.index, \"final_faiss_index.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82d10dcf-52fb-4253-9d97-c3fb68537ba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:49:35.965019Z",
     "iopub.status.busy": "2024-12-06T02:49:35.964210Z",
     "iopub.status.idle": "2024-12-06T02:49:35.973802Z",
     "shell.execute_reply": "2024-12-06T02:49:35.973263Z",
     "shell.execute_reply.started": "2024-12-06T02:49:35.964984Z"
    }
   },
   "outputs": [],
   "source": [
    "# 파일에서 FAISS 인덱스 로드\n",
    "index = faiss.read_index(\"final_faiss_index.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80437b93",
   "metadata": {},
   "source": [
    "## 5. Retriever & Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2323e06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:52:56.084439Z",
     "iopub.status.busy": "2024-12-06T02:52:56.084107Z",
     "iopub.status.idle": "2024-12-06T02:53:01.044607Z",
     "shell.execute_reply": "2024-12-06T02:53:01.043796Z",
     "shell.execute_reply.started": "2024-12-06T02:52:56.084422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'aspect': 'Governance', 'kpi': 'ESG Management', 'topic': 'ESG Strategy', 'quantity': None, 'search_terms': ['ESG', 'Governance', 'Strategy', 'Jeju Air'], 'source': '/home/jovyan/samchully/jhn/esg_inc/(주) 제주항공 2024 지속가능경영보고서.pdf', 'page': 13}, page_content='ESG\\n전략방향\\n내∙외부 이해관계자 행복 추구\\n 경영 활동 내 위험 요소 최소화\\n 친환경 비즈니스 포트폴리오 강화\\n내∙외부 이해관계자 행복 추구 투명한 거버넌스  생태계 구축 환경경영체계 구축 UN SDGs\\n연계')]\n"
     ]
    }
   ],
   "source": [
    "retriever = FAISS.from_documents(split_documents,embeddings).as_retriever(search_kwargs ={'k': 1})\n",
    "query = \"ESG 성과 지표에 대해 알려줘 json 형식으로\"\n",
    "docs = retriever.invoke(query)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2ff0c33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:53:02.034056Z",
     "iopub.status.busy": "2024-12-06T02:53:02.033547Z",
     "iopub.status.idle": "2024-12-06T02:53:02.038785Z",
     "shell.execute_reply": "2024-12-06T02:53:02.038225Z",
     "shell.execute_reply.started": "2024-12-06T02:53:02.034017Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42fcc32b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T02:53:03.452169Z",
     "iopub.status.busy": "2024-12-06T02:53:03.451778Z",
     "iopub.status.idle": "2024-12-06T02:56:29.776712Z",
     "shell.execute_reply": "2024-12-06T02:56:29.775549Z",
     "shell.execute_reply.started": "2024-12-06T02:53:03.452131Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# 모델 초기화\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# 상위 3개의 문서 선택\n",
    "compressor = CrossEncoderReranker(model=model, top_n=1)\n",
    "\n",
    "# 문서 압축 검색기 초기화\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8ac2d-5f8b-48ae-a326-4443c59e7042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
